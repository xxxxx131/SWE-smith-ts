{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"docs/","title":"Index","text":""},{"location":"getting_started/","title":"SWE-smith","text":""},{"location":"getting_started/#swe-smith","title":"SWE-smith","text":"<p>SWE-smith is toolkit for training Software Engineering (SWE) agents. With SWE-smith, you can:</p> <p>Check out the installation guide to get started, then head over to the tutorials to learn about how to use SWE-smith.</p> <p>If you use SWE-smith in your work, we'd greatly appreciate a citation:</p> <pre><code>@misc{yang2025swesmith,\n  title={SWE-smith: Scaling Data for Software Engineering Agents}, \n  author={John Yang and Kilian Lieret and Carlos E. Jimenez and Alexander Wettig and Kabir Khandpur and Yanzhe Zhang and Binyuan Hui and Ofir Press and Ludwig Schmidt and Diyi Yang},\n  year={2025},\n  eprint={2504.21798},\n  archivePrefix={arXiv},\n  primaryClass={cs.SE},\n  url={https://arxiv.org/abs/2504.21798},\n}\n</code></pre>"},{"location":"getting_started/assets/","title":"Assets","text":""},{"location":"getting_started/assets/#assets","title":"Assets","text":"<p>In addition to the paper and codebase, we release the following assets created with SWE-smith:</p> <ol> <li> <p>Environments for 128 GitHub repositories. You can download the environments (Docker images) locally by running the following command from the root directory of SWE-smith: </p><pre><code>python swesmith/build_repo/download_images.py\n</code></pre><p></p> </li> <li> <p>SWE-smith dataset of 50k+ task instances, made available as a HuggingFace dataset.</p> </li> <li> <p>5k expert trajectories + SWE-agent-LM-32B. To create <code>SWE-agent-LM-32B</code>, we fine-tuned Qwen 2.5 Coder Instruct 32B on the 5k trajectories. <code>SWE-agent-LM-32B</code> achieves 40.2% pass@1 on SWE-bench Verified. The trajectories are uploaded to a HuggingFace dataset. We also release the 32B and 7B versions of the model.</p> </li> <li> <p>SWE-Rater-32B, a Qwen 2.5 Coder Instruct 32B model fine-tuned on human annotated ratings of a SWE-bench task instance's difficulty. We release it as a HuggingFace model.</p> </li> </ol>"},{"location":"getting_started/installation/","title":"Installation","text":""},{"location":"getting_started/installation/#installation","title":"Installation","text":"<p>For the latest stable release</p> <pre><code>pip install swesmith\n</code></pre> <p>For the latest development version</p> <pre><code>git clone https://github.com/SWE-bench/SWE-smith\ncd SWE-smith\n./setup.sh\n</code></pre> <p>If you plan to contribute to SWE-smith, please also perform:</p> <pre><code>pre-commit install\n</code></pre>"},{"location":"getting_started/quickstart/","title":"Quickstart","text":"<p>We recommend checking out the tutorials for comprehensive guidance on SWE-smith usage.</p> <p>However, if you learn more easily by playing with the code, here's sequences of scripts corresponding to different SWE-smith workflows. If you run into issues, please consult the tutorials first, then open an issue if you can't find a solution.</p>"},{"location":"getting_started/quickstart/#creating-task-instances","title":"Creating Task Instances","text":"<pre><code># Run LM rewrite strategy to produce bugs\npython -m swesmith.bug_gen.llm.modify pandas-dev__pandas.95280573 \\\n    --config_file configs/bug_gen/lm_modify.yml \\\n    --model claude-3-7-sonnet-20250219 \\\n    --n_bugs 1 \\\n    --n_workers=20\n\n# Collect all task instances into a single file for validation\npython -m swesmith.bug_gen.collect_patches logs/bug_gen/pandas-dev__pandas.95280573/\n\n# Run validation on the collected task instances\npython -m swesmith.harness.valid logs/bug_gen/pandas-dev__pandas.95280573_all_patches.json --workers 8\n\n# Gather valid task instances\npython -m swesmith.harness.gather logs/run_validation/pandas_test\n\n# Generate issues for the valid task instances\npython -m swesmith.issue_gen.generate \\\n    --dataset_path logs/run_validation/basic/pandas_test.json \\\n  --model claude-3-7-sonnet-20250219 \\\n  --n_workers=1 \\\n  --config_file configs/issue_gen/ig_v2.yaml \\\n  --experiment_id ig_v2\n</code></pre> <p>Next steps</p> <p>We provide detailed tutorials on each of these steps.</p>"},{"location":"guides/","title":"Tutorials","text":""},{"location":"guides/#tutorials","title":"Tutorials","text":"<p>Build unlimited training data and train state-of-the-art SWE-agents. This guide covers the complete workflow: from environment setup to model training and evaluation.</p>"},{"location":"guides/#prerequisites","title":"\ud83d\udccb Prerequisites","text":"<p>System Requirements</p> <p>Required: Docker Tested on: Ubuntu 22.04.4 LTS Not supported: Windows, macOS</p> <p>New to SWE-smith? Start with Installation and Quickstart.</p>"},{"location":"guides/#quick-navigation","title":"\ud83c\udfaf Quick Navigation","text":"<ul> <li> <p> Build Environments</p> <p>Create reproducible Docker images for any repository. Capture dependencies, build containers, and validate with automated testing.</p> <p> Get started</p> </li> <li> <p> Create Instances</p> <p>Generate task instances using LM prompts, procedural modifications, PR mirroring, or combined techniques. Scale to thousands of bugs.</p> <p> Generate bugs</p> </li> <li> <p> Validate &amp; Evaluate</p> <p>Filter candidates that break tests and verify proposed solutions. Built-in harnesses for validation and evaluation workflows.</p> <p> Run harnesses</p> </li> <li> <p> Generate Issue Text</p> <p>Add natural language problem statements to task instances using LM generation or alternative methods.</p> <p> Create issues</p> </li> <li> <p> Rate Difficulty \u00b7 Optional</p> <p>Classify tasks as easy/medium/hard using a fine-tuned Qwen 2.5 Coder model. Compare against SWE-bench benchmarks.</p> <p> Assess difficulty</p> </li> <li> <p> Train SWE-agents</p> <p>Complete RSFT pipeline: generate trajectories, filter successful solutions, fine-tune models, and evaluate on SWE-bench.</p> <p> Start training</p> </li> </ul>"},{"location":"guides/#recommended-workflow","title":"Recommended Workflow","text":"<pre><code>graph LR\n    A[Build Environments] --&gt; B[Create Instances]\n    B --&gt; C[Validate &amp; Evaluate]\n    C --&gt; D[Generate Issue Text]\n    D --&gt; E[Rate Difficulty]\n    D --&gt; F[Train SWE-agents]\n    E --&gt; F</code></pre> <ol> <li>Build Environments \u2192 Set up Docker images</li> <li>Create Instances \u2192 Generate synthetic bugs</li> <li>Validate &amp; Evaluate \u2192 Filter valid task instances</li> <li>Generate Issue Text \u2192 Add problem descriptions</li> <li>Rate Difficulty (optional) \u2192 Classify task complexity</li> <li>Train SWE-agents \u2192 Fine-tune models with RSFT</li> </ol>"},{"location":"guides/create_instances/","title":"Create Instances","text":"<p>You should now have an environment (Docker image) of a repository. Let's synthesize some task instances.</p> <p>Task instances are modified versions of the codebase which break existing tests. The formulation for these task instances is identical to the definition put forth by SWE-bench.</p> <p>Bugs and task instances</p> <p>The terms \"bug\" and \"task instance\" are used interchangeably in this documentation. They refer to the same thing.</p> <p>We design each method with the following principles in mind:</p> <ul> <li>Generalizability: These methods work for any Python repository.</li> <li>Scalability: These methods require little to no manual intervention.</li> <li>Diversity: These methods generate a diverse set of bugs.</li> </ul> <p>As input, each method takes in the name of a GitHub repository, along with method-specific flags.</p> <p>As output, each method produces artifacts under <code>logs/bug_gen/&lt;repo&gt;</code>. A candidate task instance is usually represented as two files:</p> <ol> <li>A <code>bug__&lt;bug_type&gt;__&lt;hash&gt;.diff</code> file that, when applied to the repository, introduces a bug.</li> <li>A <code>metadata__&lt;bug_type&gt;__&lt;hash&gt;.json</code> file containing metadata about how the task instance was created.</li> </ol> <p>The <code>bug_*.diff</code> files are the candidate task instances. The <code>&lt;hash&gt;</code> is computed over the contents of the <code>bug__&lt;bug_type&gt;__&lt;hash&gt;.diff</code> file. The <code>metadata_*.json</code> files are created for statistical and analytical purposes.</p>"},{"location":"guides/create_instances/#lm-generated","title":"LM Generated","text":"<p>How does it work?</p> <p>We first identify all unique programmatic entities (e.g., classes, functions) in a repository. Per entity, we prompt a language model to either:</p> <ul> <li>Modify the entity such that a bug is introduced (a.k.a. LM Modify) or</li> <li>Rewrite the entity from scratch (a.k.a. LM Rewrite).</li> </ul> <p>How do I run it? To prompt an LM to modify a function to introduce a bug: </p><pre><code>python -m swesmith.bug_gen.llm.modify $repo \\\n  --n_bugs 1 \\\n  --model openai/gpt-4o \\\n  --config_file configs/bug_gen/lm_modify.yml\n</code></pre><p></p> <p>To prompt an LM to rewrite a function from scratch: </p><pre><code>python -m swesmith.bug_gen.llm.rewrite $repo \\\n  --model anthropic/claude-3-7-sonnet-20250219 \\\n  --config_file configs/bug_gen/lm_rewrite.yml \\\n  --n_workers 1\n</code></pre><p></p> <p>What artifact(s) does it produce? Under <code>logs/bug_gen/&lt;repo&gt;</code>, you will find a 2 dimensional folder structure.</p> <ul> <li>The first dimension corresponds to a file path in that repository (e.g., <code>&lt;repo&gt;__src__utils__rule_utils.py</code>, which would correspond to <code>src/utils/rule_utils.py</code>).</li> <li>The second dimension corresponds to individual functions or classes in that file.</li> </ul> <p>Under each of these folders, you will see:</p> <ul> <li>For LM Modify, <code>bug__lm_modify__&lt;hash&gt;.diff</code> + <code>metadata__lm_modify__&lt;hash&gt;.json</code> files</li> <li>For LM Rewrite, <code>bug__lm_rewrite__&lt;hash&gt;.diff</code> + <code>metadata__lm_rewrite__&lt;hash&gt;.json</code> files</li> </ul>"},{"location":"guides/create_instances/#procedural-modification","title":"Procedural Modification","text":"<p>How does it work?</p> <p>Similar to LM Generated bugs, we first identify all unique programmatic entities in a repository. Next, we convert every entity into an <code>ast</code> (Abstract Syntax Tree) object.</p> <p>We then modify the AST. For each programmatic entity, we check whether it satisfies a set of criteria that checks whether the entity can be modified in a certain way. If the criteria is met, we modify the AST, then convert it back into source code.</p> <p>A concrete example: We create a Procedural Modification technique that removes <code>if</code> conditional blocks from a function. We check whether a function has an <code>if</code> block in it. If it does, we remove the <code>if</code> subtree from the AST, then convert the modified AST back into code.</p> <p>Why AST's instead of the literal code? Because AST's give us a rigorous representation that allow modifications of specific program structures to be enforced easily and effortlessly.</p> <p>How do I run it? </p><pre><code>python -m swesmith.bug_gen.procedural.generate $repo \\\n  --max_bugs 10\n</code></pre><p></p> <p>What artifact(s) does it produce? Identical to LM Generated task instances, but the artifacts are instead named:</p> <ul> <li><code>bug__func_pm_&lt;name&gt;__&lt;hash&gt;.diff</code>, for instance <code>bug__func_pm_ctrl_shuffle__jdinra8l.diff</code>.</li> <li><code>metadata__func_pm_&lt;name&gt;__&lt;hash&gt;.json</code></li> </ul> <p>Where <code>&lt;name&gt;</code> refers to the specific identifier for the procedural modification technique. There are 13 (and counting) in total.</p>"},{"location":"guides/create_instances/#running-at-scale-on-modal","title":"Running at Scale on Modal","text":"<p>For generating and validating bugs across many repositories, use the <code>scripts/bug_gen.py</code> script which runs on Modal for scalable cloud execution.</p> <p>Basic Usage</p> <pre><code># Generate + validate bugs for all JavaScript repos\nmodal run scripts/bug_gen.py --language javascript\n\n# Process specific repos only\nmodal run scripts/bug_gen.py --language javascript --repos \"owner/repo1,owner/repo2\"\n</code></pre> <p>Two-Phase Pipeline</p> <p>The script runs two phases automatically:</p> <ol> <li>Generation Phase: Creates bug candidates for each repository using procedural modifications.</li> <li>Validation Phase: Runs pre-gold (baseline) tests, then post-gold tests to verify which bugs actually break tests. Results are persisted to the Modal Volume.</li> </ol> <p>Incremental Processing</p> <p>The script automatically skips already-processed repos and patches. You can safely re-run the command to continue from where it left off.</p> <p>Monitor Progress</p> <pre><code># Check current stats without running generation/validation\nmodal run scripts/bug_gen.py --language javascript --show-stats\n</code></pre> <p>This displays a table with generated, validated, and valid bug counts per repository.</p> <p>Logging to File</p> <p>To save output to a log file with real-time unbuffered streaming:</p> <pre><code>PYTHONUNBUFFERED=1 stdbuf -oL -eL uv run modal run --detach scripts/bug_gen.py --language javascript 2&gt;&amp;1 | tee validation.log\n</code></pre>"},{"location":"guides/create_instances/#pr-mirroring","title":"PR Mirroring","text":"<p>How does it work?</p> <p>This method leverages SWE-bench's task collection script.</p> <p>Run the script for a repository, and it will create a <code>&lt;repo&gt;-task-instances.jsonl.all</code>. This file contains candidate task instances based on real pull requests (PRs) from the repository. A pull request is considered a candidate if</p> <ul> <li>It has at least 1+ issue associated with it.</li> <li>It edits at least 1+ code file.</li> </ul> <p>Candidate criteria</p> <p>SWE-bench has slightly more stringent criteria for PRs that qualify as candidates. Specifically, PRs must also change 1+ test file(s). Because SWE-smith does rely on test file changes to identify breaking existing tests, we can attempt to recreate a broader subset of PRs than what SWE-bench normally considers.</p> <p>We provide this file to SWE-smith. Per PR, we ask an LM to revert the PR's changes file by file. If this process succeeds, we create a candidate task instance that effectively undoes the PR.</p> <p>How do I run it? </p><pre><code>python -m swesmith.bug_gen.mirror.generate $file \\\n    --model openai/o3-mini\n</code></pre><p></p> <p>What artifact(s) does it produce?</p>"},{"location":"guides/create_instances/#combining-bugs","title":"Combining Bugs","text":"<p>How does it work?</p> <p>You might have noticed that Procedural Modification and LM Generated bugs both target individual entities. This means at most one file is modified per bug.</p> <p>We can create more complex bugs by simply combining multiple bugs together.</p> <p>We generally do this by identifying patches that modify the same file or module. A module refers to subdirectories in the codebase.</p> <p>For the identified patches, we apply them one by one. If they all apply cleanly, we combine them into a single patch.</p> <p>You can control</p> <ul> <li><code>num_patches</code>: The number of patches to combine together.</li> <li><code>limit_per_file/module</code>: Maximum number of combined bugs to generate for any file/module.</li> <li><code>max_combos</code>: Maximum number of candidate combinations to generate.</li> <li>(For Module) <code>depth</code>: A depth of <code>2</code> means any patches under <code>a/b</code> are considered to be in the same module. A depth of <code>3</code> means any patches under <code>a/b/c</code> are considered to be in the same module.</li> </ul> <p>Validated task instances only</p> <p>This method combines validated task instances. For this method to work, you must have 1. Generated 1+ Procedural Modification or LM Modify/Rewrite bugs, and 2. validated them. See the Validation section for more details.</p> <p>How do I run it? </p><pre><code>python -m swesmith.bug_gen.combine.same_file logs/bug_gen/&lt;repo&gt; \\\n  --num_patches 3 \\\n  --limit_per_file 15 \\\n  --max_combos 100\n</code></pre><p></p> <pre><code>python -m swesmith.bug_gen.combine.same_module logs/bug_gen/&lt;repo&gt; \\\n  --num_patches 2 \\\n  --limit_per_module 20 \\\n  --max_combos 200 \\\n  --depth 2\n</code></pre> <p>What artifact(s) does it produce?</p> <p>For <code>combine.same_file</code>, a <code>bug__combine_file__&lt;hash&gt;.diff</code> file is written to <code>logs/bug_gen/&lt;repo&gt;/&lt;file_path&gt;/</code>.</p> <p>For <code>combine.same_module</code>, a <code>bug__combine_module__&lt;hash&gt;.diff</code> file is written to <code>logs/bug_gen/&lt;repo&gt;/combine_module/</code>.</p>"},{"location":"guides/difficulty_rating/","title":"Rate Difficulty","text":"<p>To see how SWE-smith compares against real world tasks (e.g. SWE-bench), we LoRA Fine-Tuned a Qwen 2.5 32B Coder Instruct model on 1.5k human ratings of the difficulty of real world bugs.</p> <p>Given the issue text and patch associated with a task instance, the model will rate the task as \"easy\" (&lt; 15 min), \"medium\" (15 min - 1 hour), or \"hard\" (1+ hours).</p>"},{"location":"guides/difficulty_rating/#inference","title":"Inference","text":"<p>You can rate the difficulty of your own task instances by following these steps:</p> <ol> <li> <p>Download the HuggingFace checkpoint.</p> </li> <li> <p>Use <code>sglang</code> to serve the checkpoint. The training scripts available in the SWE-smith repository use Modal as a compute service for hosting inference.</p> </li> </ol> <pre><code>N_HOURS=4 N_GPUS=4 modal run --detach swesmith/train/serve_sglang.py \\\n    --model-path /path/to/checkpoint \\\n    --served-model-name gpt-4o \\\n    --tokenizer-path /path/to/Qwen2.5-Coder-32B-Instruct\n</code></pre> <ol> <li>Run the following script:</li> </ol> <pre><code>python swesmith/train/difficulty_rater/get_difficulties.py \\\n    --base_url &lt;URL where model is hosted&gt; \\\n    --dataset_path path/to/dataset.json\n</code></pre> <p>The script will generate a <code>.json</code> file containing a mapping from each task instance to a difficulty score. You can then compute the dataset's difficulty score as the average of all task instance scores.</p>"},{"location":"guides/difficulty_rating/#prior-datasets","title":"Prior Datasets","text":"<p>Using our model, we've assessed the difficulty of existing datasets, assigning scores of 1/5/9 to easy/medium/hard tasks.</p> Dataset # Instances Score <code>easy</code> <code>med</code> <code>hard</code> SWE-bench 2294 5.014 438 1408 446 \u2514\u2500\u2500 Lite 300 3.893 93 197 10 \u2514\u2500\u2500 Verified 500 3.960 173 284 43 SWE-bench Multimodal 510 6.036 55 265 186 SWE-gym 2438 5.625 288 1456 664 \u2514\u2500\u2500 Lite 230 3.890 67 156 4 SWE-smith (LM Modify) 1000 3.304 441 542 17 SWE-smith (LM Rewrite) 1000 5.272 68 796 136 SWE-smith (Procedural) 1000 3.596 374 603 23 SWE-smith (PR Mirror) 1000 4.876 206 619 175 SWE-smith (Combine) 1000 5.720 52 716 232 <p>From the table, we demonstrate that SWE-smith task instances are comparable to real world tasks, and that our bug generation techniques allow for a wide range of task difficulties.</p>"},{"location":"guides/env_construction/","title":"Build Environments","text":""},{"location":"guides/env_construction/#build-environments","title":"Build Environments","text":"<p>SWE-smith enables automatic conversion of code repositories into reinforcement learning environments.</p> <p>We'll review the two steps of this process:</p> <ol> <li>SWE-agent + LM attempts to install a repository + run the testing suite.</li> <li>Construct an execution environment (Docker image).</li> </ol> <p>For this section, we'll use the Instagram/MonkeyType repository as a running example,  specifically at commit <code>70c3acf</code>.</p>"},{"location":"guides/env_construction/#automatically-install-repos-with-swe-agent","title":"Automatically Install Repos with SWE-agent","text":"<p>Coming soon!</p> <p>Python installation scripts</p> <p>Early on in SWE-smith's development, we focused exclusively on Python repositories and wrote Python-specific scripts for automatic repo instllation. More information here</p>"},{"location":"guides/env_construction/#create-an-execution-environment","title":"Create an Execution Environment","text":"<p>Run the following command to create a Docker image for the repository.</p> <pre><code>python -m swesmith.build_repo.create_images -r MonkeyType\n</code></pre> <p>This command will create two artifacts: 1. A mirror of the original repository at the specified commit, created under <code>swesmith</code>. To change the organization, you can...     * Pass in an <code>--org</code> argument, or     * (If built from source) Change <code>ORG_NAME_GH</code> in <code>swesmith/constants.py</code> 2. A Docker image (<code>swesmith.x86_64.&lt;repo&gt;.&lt;commit&gt;</code>) which contains the installed codebase.</p> <p><code>create_images</code> arguments</p> <p>By default, without <code>-r</code>, the command will build images for all SWE-smith repositories (300+ as of 12/2025).</p> <p><code>-r</code>: Select specific repositories to build using fuzzy matching (e.g., <code>-r django</code> matches any repo containing \"django\").</p> <p><code>-f</code>: Force rebuild images even if they already exist locally.</p> <p>It's good practice to check that your Docker image works as expected. </p><pre><code>docker run -it --rm swebench/swesmith.x86_64.instagram__monkeytype.70c3acf6\n</code></pre> Within the container, run the testing suite (e.g. <code>pytest</code>) to ensure that the codebase is functioning as expected.<p></p> <p>Get existing Docker images</p> <p>All repositories represented in the SWE-smith dataset are available to download. Simply run: </p><pre><code>python -m swesmith.build_repo.download_images\n</code></pre><p></p>"},{"location":"guides/env_construction_py/","title":"Build Python Environments","text":""},{"location":"guides/env_construction_py/#python-environment-options","title":"Python Environment Options","text":"<p>This guide covers Python-specific options for customizing repository installation when using <code>try_install_py</code>.</p>"},{"location":"guides/env_construction_py/#quickstart","title":"Quickstart","text":"<p>The following is a quick demonstration for how to use the <code>try_install_py</code> script. For more details, refer to the \"Running Example\" section.</p> <p></p><pre><code>python -m swesmith.build_repo.try_install_py Instagram/MonkeyType configs/install_repo.sh \\\n    --commit 70c3acf62950be5dfb28743c7a719bfdecebcd84\n</code></pre> where <code>install_repo.sh</code> is the script that installs the repository. (Example)<p></p> <p>If successful, two artifacts will be produced under <code>logs/build_repo/env/&lt;org&gt;__&lt;repo&gt;.&lt;hash&gt;</code>:</p> <ul> <li><code>sweenv_[repo + commit].yml</code>: A dump of the conda environment that was created.</li> <li><code>sweenv_[repo + commit].sh</code>: A log of the installation process.</li> </ul>"},{"location":"guides/env_construction_py/#walkthrough","title":"Walkthrough","text":"<p>Throughout this guide, we'll use the Instagram/MonkeyType repository at commit <code>70c3acf</code> as our running example.</p>"},{"location":"guides/env_construction_py/#python-version-selection","title":"Python Version Selection","text":"<p>By default, the installation script creates a conda environment with Python 3.10. You can specify a different Python version using the <code>--python-version</code> flag:</p> <pre><code>python -m swesmith.build_repo.try_install_py Instagram/MonkeyType configs/install_repo.sh \\\n    --commit 70c3acf62950be5dfb28743c7a719bfdecebcd84 \\\n    --python-version 3.11\n</code></pre> <p>This sets the <code>SWESMITH_PYTHON_VERSION</code> environment variable, which the install script uses when creating the conda environment:</p> <pre><code>conda create -n testbed \"python=${PYTHON_VERSION}\" -yq\n</code></pre>"},{"location":"guides/env_construction_py/#test-dependency-installation","title":"Test Dependency Installation","text":"<p>The install script now tries multiple strategies to install test dependencies, in order:</p> <ol> <li>Extras: <code>pip install -e \".[test]\"</code></li> <li>Requirements file: <code>pip install -r requirements-test.txt</code></li> <li>Profile hooks: Custom install commands from the repository profile</li> </ol> <p>For MonkeyType, the install script will successfully use the extras approach since <code>setup.py</code> defines test dependencies in <code>extras_require</code>.</p> <p>Adding Extra Test Dependencies. If a repository needs additional test utilities beyond its declared dependencies, use <code>--extra-test-deps</code>:</p> <pre><code>python -m swesmith.build_repo.try_install_py Instagram/MonkeyType configs/install_repo.sh \\\n    --commit 70c3acf62950be5dfb28743c7a719bfdecebcd84 \\\n    --extra-test-deps \"hypothesis coverage\"\n</code></pre> <p>This passes the packages to the install script via <code>SWESMITH_EXTRA_TEST_DEPS</code>, which installs them after the main test dependencies:</p> <pre><code>python -m pip install hypothesis coverage\n</code></pre>"},{"location":"guides/env_construction_py/#smoke-testing","title":"Smoke Testing","text":"<p>After installation, the script runs a smoke test to verify the environment works correctly. This flag is not necessary, but provided to enable easy checking. In general, we encourage putting all necessary installation steps in <code>install_repo.sh</code>.</p> <p>Default Behavior. If pytest is available in the environment, the default smoke test is: </p><pre><code>pytest -q --maxfail=1\n</code></pre><p></p> <p>For MonkeyType, this would run: </p><pre><code>&gt; Running smoke test: pytest -q --maxfail=1\n&gt; Smoke test passed\n</code></pre><p></p> <p>Custom Smoke Test. You can specify a custom smoke test command:</p> <pre><code>python -m swesmith.build_repo.try_install_py Instagram/MonkeyType configs/install_repo.sh \\\n    --commit 70c3acf62950be5dfb28743c7a719bfdecebcd84 \\\n    --smoke-cmd \"python -m pytest tests/test_cli.py -v\"\n</code></pre> <p>Skipping Smoke Test. To skip the smoke test entirely:</p> <pre><code>python -m swesmith.build_repo.try_install_py Instagram/MonkeyType configs/install_repo.sh \\\n    --commit 70c3acf62950be5dfb28743c7a719bfdecebcd84 \\\n    --skip-smoke\n</code></pre>"},{"location":"guides/env_construction_py/#debugging-options","title":"Debugging Options","text":"<p>Force Overwrite. If the environment file already exists, you'll be prompted to overwrite it. Use <code>--force</code> to skip the prompt:</p> <pre><code>python -m swesmith.build_repo.try_install_py Instagram/MonkeyType configs/install_repo.sh \\\n    --commit 70c3acf62950be5dfb28743c7a719bfdecebcd84 \\\n    --force\n</code></pre> <p>Keep Environments for Inspection. By default, the script cleans up the cloned repository and conda environment after completion. Use <code>--no_cleanup</code> to keep them for debugging:</p> <pre><code>python -m swesmith.build_repo.try_install_py Instagram/MonkeyType configs/install_repo.sh \\\n    --commit 70c3acf62950be5dfb28743c7a719bfdecebcd84 \\\n    --no_cleanup\n</code></pre> <p>After running with this flag, you can inspect the environment:</p> <pre><code>conda activate testbed\ncd MonkeyType\npytest tests/\n</code></pre>"},{"location":"guides/env_construction_py/#complete-example","title":"Complete Example","text":"<p>Here's a complete example using MonkeyType with multiple options:</p> <pre><code>python -m swesmith.build_repo.try_install_py Instagram/MonkeyType configs/install_repo.sh \\\n    --commit 70c3acf62950be5dfb28743c7a719bfdecebcd84 \\\n    --python-version 3.11 \\\n    --extra-test-deps \"hypothesis\" \\\n    --smoke-cmd \"pytest tests/test_cli.py -q\" \\\n    --force\n</code></pre> <p>This will: - Clone MonkeyType at commit <code>70c3acf</code> - Create a conda environment with Python 3.11 - Install the repository in editable mode - Install test dependencies (found via <code>[test]</code> extras) - Install <code>hypothesis</code> as an additional test dependency - Run <code>pytest tests/test_cli.py -q</code> as the smoke test - Force overwrite any existing environment file - Clean up the repository and environment after completion</p>"},{"location":"guides/harnesses/","title":"Validation & Evaluation","text":""},{"location":"guides/harnesses/#validation-evaluation","title":"Validation &amp; Evaluation","text":"<p>Great! You now have an execution environment + a bunch of candidate task instances. How do we determine which ones can be used for training?</p> <p>We provide two harnesses for the purposes of:</p> <ul> <li>Validation: To check if a candidate task instance is usable (breaks 1+ existing tests).</li> <li>Evaluation: To check if the proposed solution for a task instance is correct.</li> </ul> <p>The purposes of these harnesses are identical to their motivations in SWE-bench.</p>"},{"location":"guides/harnesses/#validation","title":"Validation","text":"<p>The validation harness is used to check if a candidate task instance is usable (breaks 1+ existing tests).</p> <p>Once you've generated task instance candidates, follow these steps to validate them:</p> <ol> <li>Collect the candidates</li> </ol> <pre><code>python -m swesmith.bug_gen.collect_patches logs/bug_gen/&lt;repo&gt;\n</code></pre> <p>This produces a <code>logs/bug_gen/&lt;repo&gt;_all_patches.json</code> file with all the candidate task instances.</p> <ol> <li>Run validation</li> </ol> <pre><code>python -m swesmith.harness.valid logs/bug_gen/&lt;repo&gt;_all_patches.json\n</code></pre> <p>The validation harness works in two steps. First, it runs the original repository's test suite to get the passing statuses of the existing tests. Then, it applies each candidate task instance to the repository and runs the test suite again. If the candidate task instance breaks 1+ existing tests, it is considered a usable task instance.</p> <p>For each task instance, the validation harness produces a <code>logs/run_validation/&lt;run_id&gt;/&lt;instance_id&gt;</code> folder containing the following information:</p> <ul> <li><code>eval.sh</code>: The sequence of test command(s) run</li> <li><code>patch.diff</code>: The candidate task instance</li> <li><code>report.json</code>: <code>FAIL_TO_PASS</code> and <code>PASS_TO_PASS</code> test cases</li> <li><code>run_instance.log</code>: The full trace of running validation</li> <li><code>test_output.txt</code>: The standard output of the test command(s)</li> </ul> <ol> <li>Collect validated task instances</li> </ol> <pre><code>python -m swesmith.harness.gather logs/run_validation/&lt;run_id&gt;\n</code></pre> <p>Task instances with 1+ <code>FAIL_TO_PASS</code> test cases and 1+ <code>PASS_TO_PASS</code> test cases are considered valid.</p> <p>This script performs two actions:</p> <ul> <li>It collects all valid task instances into a <code>logs/task_insts/&lt;run_id&gt;.json</code>. Each instance contains the following information: <pre><code>{\n    \"instance_id\": &lt;instance_id&gt;,\n    \"repo\": &lt;repo&gt;,\n    \"patch\": &lt;The diff that, when applied, creates the bug&gt;,\n    \"FAIL_TO_PASS\": &lt;List of broken test cases&gt;,\n    \"PASS_TO_PASS\": &lt;List of passing test cases&gt;,\n    \"image_name\": &lt;docker image name&gt;,\n}\n</code></pre></li> <li>For each valid task instance, a branch called <code>&lt;instance_id&gt;</code> is created in the repository. The branch corresponds to the repository with the task instance's bug patch applied.</li> </ul>"},{"location":"guides/harnesses/#evaluation","title":"Evaluation","text":"<p>The evaluation harness is used to check if the proposed solution for a task instance is correct.</p> <p>You can run this script to sanity check that testing for validated task instances works as expected:</p> <pre><code>python -m swesmith.harness.eval \\\n    --dataset_path bugs/task_insts/{repo}.json \\\n    --predictions_path gold \\\n    --run_id sanity\n</code></pre> <p>If you want to run on real predictions, simply replace <code>gold</code> with the path to your predictions, which should look like:</p> <pre><code>{\n    \"instance_id\": &lt;instance_id&gt;,\n    \"patch\": &lt;The diff that, when applied, fixes the bug&gt;,\n    \"model_name_or_path\": &lt;The model used to generate the patch&gt;,\n}\n</code></pre>"},{"location":"guides/issue_gen/","title":"Generate Issue Text","text":"<p>You have a bunch of task instances with executable environments. You're very close to training SWE-agents on this data. There's one last step - let's generate issue text.</p> <p>We primarily use LM's to generate issue text.</p> <pre><code>python swesmith/issue_gen/generate.py logs/task_insts/&lt;repo&gt;.json \\\n    --config_file configs/issue_gen/ig_v2.yaml \\\n    --model anthropic/claude-3-7-sonnet-20250219 \\\n    --n_workers 4 \\\n    --experiment_id ig_v2 \\\n    --use_existing\n</code></pre> <p>This will generated issue text for each task instance, producing several artifacts along the way:</p> <ul> <li>Under <code>logs/issue_gen/ig_v2/&lt;repo&gt;</code>, there will be a folder for each task instance, containing:<ul> <li><code>messages.json</code>: The messages fed to the LM to generate the issue text.</li> <li><code>metadata.json</code>: Conatins the issue text + inference cost.</li> </ul> </li> <li>In the same directory as <code>logs/task_insts/&lt;repo&gt;.json</code>, a <code>logs/issue_gen/&lt;repo&gt;__ig_v2_n1.json</code> file will be created, which is a copy of the original file with issue text added to each task instance (as the <code>problem_statement</code> field).</li> </ul>"},{"location":"guides/issue_gen/#alternatives","title":"Alternatives","text":"<p>In our paper, we discuss several alternatives for generating issue text. While our experiments suggest that LM generated issue text is the best proxy for real issue text, we provide instructions for the alternatives below.</p> <p>Static Issue Text</p> <p>The problem statement is generated by randomly selecting one of 7 static issue text templates.</p> <pre><code>python swesmith/issue_gen/get_static.py logs/task_insts/&lt;repo&gt;.json\n</code></pre> <p>Produces a <code>logs/issue_gen/&lt;repo&gt;__ig_static.json</code> file.</p> <p>Random F2P Test Case</p> <p>The problem statement shows a randomly selected Fail-to-Pass test case from the task instance.</p> <pre><code>python swesmith/issue_gen/get_from_tests.py logs/task_insts/&lt;repo&gt;.json\n</code></pre> <p>Original Issue Text</p> <p>Note</p> <p>This strategy only works for some PR Mirrors, if the pull request the mirror is based on has issue(s) associated with it.</p> <pre><code>python swesmith/issue_gen/get_from_pr.py logs/task_insts/&lt;repo&gt;.json\n</code></pre> <p>Produces a <code>logs/issue_gen/&lt;repo&gt;__ig_orig.json</code> file.</p>"},{"location":"guides/train_swe_agent/","title":"Train SWE-agents","text":""},{"location":"guides/train_swe_agent/#training-swe-agents","title":"Training SWE-agents","text":"<p>Now the fun part - we provide details on how to operationalize SWE-smith for training SWE-agents!</p> <p>Specifically, we'll cover the workflow for Rejection Sampling Fine Tuning.</p> <p>SWE-agent</p> <p>The documentation in this section is heavily grounded in the SWE-agent library. We do not plan to explicitly support non SWE-agent scaffolds, but it should not be difficult - the main adaptations would just be how you generate expert trajectories and predictions for evaluation.</p> <p>There's several steps we'll cover:</p> <ol> <li>Creating a subset of SWE-smith task instances.</li> <li>Generating expert trajectories for those task instances.</li> <li>Training a model on the expert trajectories.</li> <li>Evaluating the model on SWE-bench (Lite/Verified/Multimodal).</li> </ol>"},{"location":"guides/train_swe_agent/#creating-swe-smith-subset","title":"Creating SWE-smith Subset","text":"<p>If you are using SWE-smith, the dataset of all SWE-smith is quite large. Usually, we recommend training on a subset. To curate a subset, you might use the following logic.</p> <pre><code>import json\n\nfrom datasets import load_dataset\nswesmith = load_dataset(\"SWE-bench/SWE-smith\", split=\"train\")\n\nsubset_name = \"subset0\"\ndef criteria(task_instance):\n    return \".pr_\" in task_instance[\"instance_id\"] and \\\n        len(task_instance[\"FAIL_TO_PASS\"]) &lt;= 5 and \\\n        len(task_instance[\"FAIL_TO_PASS\"]) &gt;= 2\nbugs = [x for x in swesmith if criteria(x)]\nprint(f\"Found {len(bugs)} bugs that match criteria\")\nwith open(f\"logs/experiments/{subset_name}.json\", \"w\") as f:\n    json.dump(bugs, fp=f, indent=2)\n</code></pre>"},{"location":"guides/train_swe_agent/#generate-expert-trajectories","title":"Generate Expert Trajectories","text":"<ol> <li> <p>Clone SWE-agent. Make sure to follow the installation instructions here.</p> </li> <li> <p>Create a soft link of the <code>agent/</code> folder to SWE-agent, meaning in SWE-agent, run: </p><pre><code>ln -s path/to/SWE-smith/agent/ .\n</code></pre><p></p> </li> <li> <p>In SWE-agent, run exeprt trajectory generation: </p><pre><code>./agent/_gen_trajs.sh\n</code></pre> Check the file to see how the script works. You'll need to adjust the <code>--instances.path</code> argument to point to the subset you created in the previous step.<p></p> </li> </ol>"},{"location":"guides/train_swe_agent/#train-model","title":"Train Model","text":"<p>The previous step will generate individual trajectories per task instance under the <code>SWE-agent/trajectories/&lt;username&gt;/&lt;run ID&gt;/</code> folder.</p> <p>We'll now determine which trajectories correspond to resolved instances, convert them to a format that can be used for SFT, and then train a model with them.</p> <ol> <li>(From SWE-smith) Run evaluation on training task instances. <pre><code>python -m swesmith.harness.eval \\\n    --dataset_path path/to/subset0.json \\\n    --predictions_path path/to/trajectories/&lt;username&gt;/&lt;run ID&gt;/preds.json \\\n    --run_id &lt;run ID&gt; \\\n    --workers 10 \\\n    --timeout 240\n</code></pre></li> </ol> <p><code>preds.json</code></p> <p>If there is no <code>preds.json</code>, run <code>sweagent merge-preds trajectories/&lt;username&gt;/&lt;run ID&gt;/</code>.</p> <p>This evaluation will generate a <code>logs/run_evaluation/&lt;run ID&gt;/</code> folder with a <code>report.json</code> file indicating which instance IDs were successfully resolved.</p> <ol> <li>(From SWE-smith) Convert trajectories into SFT format.</li> </ol> <pre><code>python -m swesmith.train.traj_mgr.collect_trajs \\\n    --traj_dir path/to/trajectories/&lt;username&gt;/&lt;run ID&gt;/ \\\n    --eval_dir logs/run_evaluation/&lt;run ID&gt;/\n</code></pre> <p>This will product an <code>ft_xml_*.jsonl</code> file under the <code>trajectories_sft/</code> folder. This dataset can be used directly for SFT.</p> <ol> <li>Run training. First, upload the file to Modal <pre><code>modal volume put &lt;volume&gt; trajectories_sft/ft_xml_*.jsonl\n</code></pre></li> </ol> <p>Then, modify <code>config/train/full_ft_qwen_7b.yml</code> to point to the file in Modal.</p> <p>Finally, run the training script: </p><pre><code>./scripts/train.run_ft_torchtune.py\n</code></pre><p></p>"},{"location":"guides/train_swe_agent/#evaluation","title":"Evaluation","text":"<p>Run inference on SWE-agent + your SFT'ed model on SWE-bench (Lite/Verified/Multimodal).</p> <ol> <li> <p>(From SWE-smith) Update <code>scripts/train.serve_sglang.sh</code> to point at SFT'ed model, then run it.</p> </li> <li> <p>(From SWE-agent) Run inference: </p><pre><code>./agent/_infer_model.sh\n</code></pre> Make sure the Modal URL is correct and change the evaluation dataset as desired.<p></p> </li> <li> <p>When inference finishes, run evaluation on the model's predictions. (Check out sb-cli for more information on how to conveniently run evaluation for SWE-bench-* datasets.) </p><pre><code>sb-cli submit swe-bench_verified test \\\n    --predictions_path trajectories/&lt;username&gt;/&lt;run ID&gt;/preds.json \\\n    --run_id &lt;run ID&gt;\n</code></pre><p></p> </li> </ol>"}]}